# 快速排序

## 快速排序的原理

快排利用的也是分治思想。

和归并排序的思路完全不同，如果要排序数组中下标从 p 到 r 之间的一组数据：

- 先选择 p 到 r 之间的任意一个数据作为 pivot（分区点）
- 遍历 p 到 r 之间的数据
  - 小于 pivot 的放到左边
  - 大于 pivot 的放到右边
  - pivot 放到中间

![快速排序](@imgs/4d892c3a2e08a17f16097d07ea088a81.jpg)

经过这一步骤之后，数组 p 到 r 之间的数据就被分成了三个部分，前面 p 到 q-1 之间都是小于 pivot 的，中间是 pivot，后面的 q+1 到 r 之间是大于 pivot 的。

根据分治、递归的处理思想，我们可以用递归排序下标从 p 到 q-1 之间的数据和下标从 q+1 到 r 之间的数据，直到区间缩小为 1，就说明所有的数据都有序了

。如果我们用递推公式来将上面的过程写出来的话，就是这样：

```
递推公式：
quick_sort(p, r) = quick_sort(p, q - 1) + quick_sort(q + 1, end)

终止条件：
p >= r
```

```ts
function quickSort(arr: number[]) {
  const quickSortInternal = (arr, p, r) => {
    if (p >= r) return

    const q = partition(arr, p, r)
    quickSortInternal(arr, p, q - 1)
    quickSOrtInternal(arr, q + 1, r)
  }
  const partition = (arr, p, r) => {
    const pivot = arr[p]
    let index = p + 1

    for (let i = index; i <= r; i++) {
      if (arr[i] < pivot) {
        swap(arr, index, i)
        index++
      }
    }
    swap(arr, p, index - 1)
    return index - 1
  }

  const swap = (arr, p, q) => {
    const tmp = arr[p]
    arr[p] = arr[q]
    arr[q] = tmp
  }

  quickSortInternal(arr, 0, arr.length - 1)

  return arr
}
```

## 特点

### 原地排序 - 空间复杂度

归并排序中有一个 merge() 合并函数，快排有一个 partition() 分区函数，就是随机选择一个元素作为 pivot（一般情况下，可以选择 p 到 r 区间的最后一个元素），然后对 A[p…r]分区，函数返回 pivot 的下标。

如果我们不考虑空间消耗的话，partition() 分区函数可以写得非常简单。我们申请两个临时数组 X 和 Y，遍历 A[p…r]，将小于 pivot 的元素都拷贝到临时数组 X，将大于 pivot 的元素都拷贝到临时数组 Y，最后再将数组 X 和数组 Y 中数据顺序拷贝到 A[p…r]。但是，如果按照这种思路实现的话，partition() 函数就需要很多额外的内存空间，所以快排就不是原地排序算法了。

如果我们希望快排是原地排序算法，那它的空间复杂度得是 O(1)，那 partition() 分区函数就不能占用太多额外的内存空间，我们就需要在 A[p…r]的原地完成分区操作。

这里的处理有点类似选择排序。我们通过游标 i 把 A[p…r-1]分成两部分。A[p…i-1]的元素都是小于 pivot 的，我们暂且叫它“已处理区间”，A[i…r-1]是“未处理区间”。我们每次都从未处理的区间 A[i…r-1]中取一个元素 A[j]，与 pivot 对比，如果小于 pivot，则将其加入到已处理区间的尾部，也就是 A[i]的位置。数组的插入操作还记得吗？在数组某个位置插入元素，需要搬移数据，非常耗时。当时我们也讲了一种处理技巧，就是交换，在 O(1) 的时间复杂度内完成插入操作。这里我们也借助这个思想，只需要将 A[i]与 A[j]交换，就可以在 O(1) 时间复杂度内将 A[j]放到下标为 i 的位置。

![原地排序的快排](@imgs/086002d67995e4769473b3f50dd96de7.jpg)

### 稳定性

因为分区的过程涉及交换操作，如果数组中有两个相同的元素，比如序列 6，8，7，6，3，5，9，4，在经过第一次分区操作之后，两个 6 的相对先后顺序就会改变。所以，快速排序并不是一个稳定的排序算法。

### 时间复杂度

快排也是用递归来实现的。对于递归代码的时间复杂度，我前面总结的公式，这里也还是适用的。

#### 最好情况时间复杂度

如果每次分区操作，都能正好把数组分成大小接近相等的两个小区间，那快排的时间复杂度递推求解公式跟归并是相同的，所以，快排的时间复杂度也是 O(n * logn)。

```
T(1) = C；   n=1时，只需要常量级的执行时间，所以表示为C。
T(n) = 2 * T(n/2) + n； n > 1
```

但是，公式成立的前提是每次分区操作，我们选择的 pivot 都很合适，正好能将大区间对等地一分为二。但实际上这种情况是很难实现的。

#### 最坏情况时间复杂度

举一个比较极端的例子，如果数组中的数据原来已经是有序的了，比如 1，3，5，6，8。

如果我们每次选择最后一个元素作为 pivot，那每次分区得到的两个区间都是不均等的。我们需要进行大约 n 次分区操作，才能完成快排的整个过程。

每次分区我们平均要扫描大约 n/2 个元素，这种情况下，快排的时间复杂度就从 O(nlogn) 退化成了 O(n^2)。

#### 平均情况时间复杂度

那快排的平均情况时间复杂度是多少呢？

我们假设每次分区操作都将区间分成大小为 9:1 的两个小区间。我们继续套用递归时间复杂度的递推公式，就会变成这样：

```
T(1) = C；   n = 1 时，只需要常量级的执行时间，所以表示为 C。

T(n) = T(n/10) + T( 9 * n/10) + n； n > 1
```

## 归并排序 vs 快排

快排和归并用的都是分治思想，递推公式和递归代码也非常相似，那它们的区别在哪里呢？

![归并排序和快排的区别](@imgs/aa03ae570dace416127c9ccf9db8ac05.jpg)

- 归并排序的处理过程是由下到上的，先处理子问题，然后再合并
- 快排正好相反，它的处理过程是由上到下的，先分区，然后再处理子问题

复杂度：

- 归并排序虽然是稳定的、时间复杂度为 O(nlogn) 的排序算法，但是它是非原地排序算法，主要原因是因为合并函数无法在原地执行
- 快速排序通过设计巧妙的原地分区函数，可以实现原地排序，解决了归并排序占用太多内存的问题

## 思考

### TOP K 问题

如何在 O(n) 时间复杂度内求无序数组中的第 K 大元素？

比如，4， 2， 5， 12， 3 这样一组数据，第 3 大元素就是 4。

#### 插入排序思想

借助插入排序思想，每次取数组中的最小值，将其移动到数组的最前面，然后在剩下的数组中继续找最小值，以此类推，执行 K 次，找到的数据不就是第 K 大元素。

该方法的时间复杂度并不是 O(n)，而是 O(K * n)。

当 K 是比较小的常量时，比如 1、2 那最好时间复杂度确实是 O(n)；但当 K 等于 n/2 或者 n 时，这种最坏情况下的时间复杂度就是 O(n^2) 了。所以 O(k * n) 不能简单地和 O(n) 划等号。

#### 快排思想

借助快排思想：

- 选择数组的最后一个元素作为 pivot
- 对数组 A 原地分区，这样数组就分成了三部分 A[0...p-1], A[p], A[p+1...n-1]。其中 n 为数组长度
  - 如果 p + 1 = K，那 A[p] 就是要求解的元素
  - 如果 p + 1 < K, 说明第 K 大元素出现在 A[p+1...n-1]区间，我们再按照上面的思路递归地在 A[p+1...n-1]这个区间内查找。
  - 如果 p + 1 > K，那我们就在 A[0...p-1]区间查找。

![解决思路](@imgs/898d94fc32e0a795fd65897293b98791.jpg)

为什么上述解决思路的时间复杂度是 O(n)？

- 第一次分区查找，我们需要对大小为 n 的数组执行分区操作，需要遍历 n 个元素
- 第二次分区查找，我们只需要对大小为 n/2 的数组执行分区操作，需要遍历 n/2 个元素
- 依次类推，分区遍历元素的个数分别为、n/2、n/4、n/8、n/16...直到区间缩小为 1
- 如果我们把每次分区遍历的元素个数加起来，就是：n + n/2 + n/4 + n/8 + ... + 1
- 这是一个等比数列求和，最后的和等于 2n - 1
- 所以，上述解决思路的时间复杂度就为 O(n)

### 日志文件合并

现在你有 10 个接口访问日志文件，每个日志文件大小约 300MB，每个文件里的日志都是按照时间戳从小到大排序的。你希望将这 10 个较小的日志文件，合并为 1 个日志文件，合并之后的日志仍然按照时间戳从小到大排列。如果处理上述排序任务的机器内存只有 1GB，你有什么好的解决思路，能“快速”地将这 10 个日志文件合并吗？

- 先构建十条 io 流，分别指向十个文件
- 每条 io 流读取对应文件的第一条数据，然后比较时间戳，选择出时间戳最小的那条数据，将其写入一个新的文件
- 然后指向该时间戳的 io 流读取下一行数据，然后继续刚才的操作，比较选出最小的时间戳数据，写入新文件，io 流读取下一行数据，以此类推，完成文件的合并

复杂度分析：

- 这种处理方式，日志文件有 n 个数据就要比较 n 次，每次比较选出一条数据来写入，时间复杂度是 O(n)
- 空间复杂度是O(1)

## 总结

快速排序算法虽然最坏情况下的时间复杂度是 O(n^2)，但是平均情况下时间复杂度都是 O(nlogn)。不仅如此，快速排序算法时间复杂度退化到 O(n^2) 的概率非常小，我们可以通过合理地选择 pivot 来避免这种情况。
